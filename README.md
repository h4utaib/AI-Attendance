# ğŸ”¥ RAG-powered AI Chatbot on OpenShift AI ğŸš€

![banner](https://user-images.githubusercontent.com/placeholder/banner.png)

Welcome to this cutting-edge *RAG (Retrieval-Augmented Generation)* chatbot project!  
This app combines the best of *open-source LLMs, OpenShift AI, LangChain, and vector search* to give you fast, accurate answers from your own data â€” all wrapped in a sleek Flask API!

---

## âœ¨ Tech Stack

| ğŸ”§ Component         | ğŸ’¡ Description |
|----------------------|----------------|
| ğŸ§  *Mistral LLM*   | Lightweight, high-performance open-source LLM |
| ğŸ“¦ *vLLM*          | Fast, efficient model serving engine |
| â˜ï¸ *OpenShift AI*  | Scalable deployment of AI workloads |
| ğŸ” *LangChain*     | RAG framework for retrieval + generation |
| ğŸ—ƒï¸ *Elasticsearch*| Vector DB for embedding-based search |
| ğŸŒ *Flask*         | REST API framework to interact with the model |

---

## ğŸ”§ Architecture

![architecture](https://user-images.githubusercontent.com/placeholder/architecture-diagram.png)

*Flow*:
1.â  â ğŸ“ User sends a question to Flask API.
2.â  â ğŸ” LangChain retrieves relevant chunks from Elasticsearch.
3.â  â ğŸ§  Mistral LLM (served via vLLM on OpenShift AI) generates the answer.
4.â  â ğŸš€ Response is returned to the user.

---
